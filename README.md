# AnyCompany Food & Beverage - Plateforme Analytics Data-Driven

##  Contexte Business

**AnyCompany Food & Beverage**, fabricant international de produits alimentaires et boissons premium depuis 25 ans, traverse une crise majeure :

-  **Baisse des ventes sans pr√©c√©dent** sur le dernier exercice fiscal
-  **R√©duction de 30% du budget marketing**
-  **Part de march√© chut√©e de 28% √† 22%** en 8 mois
-  Concurrence accrue de startups D2C pilot√©es par la data (prix inf√©rieurs de 5-15%)

**Mission confi√©e √† Sarah, Senior Marketing Executive** : inverser la tendance et **atteindre 32% de part de march√© d'ici T4 2025** gr√¢ce √† une strat√©gie marketing data-driven.

---

##  Architecture Technique - Lakehouse M√©daillon

Ce projet impl√©mente une architecture moderne **Bronze ‚Üí Silver ‚Üí Gold** sur Snowflake :

```
 AWS S3 (Data Lake)
    ‚Üì
 BRONZE Layer - Donn√©es brutes
    ‚îÇ ‚Ä¢ Ingestion multi-format (CSV, JSON)
    ‚îÇ ‚Ä¢ Typage fort des colonnes
    ‚îÇ ‚Ä¢ Tra√ßabilit√© compl√®te
    ‚Üì
 SILVER Layer - Donn√©es nettoy√©es
    ‚îÇ ‚Ä¢ D√©doublonnage intelligent (QUALIFY)
    ‚îÇ ‚Ä¢ Validation m√©tier (coh√©rence dates, montants positifs)
    ‚îÇ ‚Ä¢ Enrichissement (calculs d'√¢ge, dur√©es, flags)
    ‚Üì
 GOLD Layer - Data Products
    ‚îÇ ‚Ä¢ Tables analytiques d√©normalis√©es
    ‚îÇ ‚Ä¢ Features ML pr√©-calcul√©es
    ‚îÇ ‚Ä¢ KPIs m√©tier agr√©g√©s
    ‚Üì
 Dashboards Streamlit
    ‚Ä¢ Sales Performance
    ‚Ä¢ Marketing ROI
    ‚Ä¢ Promotion Analysis
```

---

## Points Diff√©renciants de ce Projet

### 1Ô∏è **SQL au C≈ìur de l'Architecture - 100% Transformations en SQL Natif**

Contrairement aux projets qui encapsulent la logique dans Python, **toutes les transformations sont en SQL Snowflake** :

‚úÖ **Window Functions avanc√©es** : moving averages, growth rates, lag/lead  
‚úÖ **CTEs complexes** : analyses multi-niveaux, cohort analysis  
‚úÖ **QUALIFY pour d√©doublonnage** : alternative moderne au `ROW_NUMBER() + WHERE`  
‚úÖ **Feature Engineering SQL** : RFM scores, lifetime value, churn indicators  
‚úÖ **Parsing JSON natif** : extraction et typage des donn√©es JSON

**Voir fichier** : `sql/5_√âvolution_ventes.sql` pour exemples de croissance mensuelle avec window functions

### 2Ô∏è **Nettoyage M√©tier Avanc√© avec Validation de Coh√©rence**

Le layer SILVER ne se contente pas de `NULL` handling - il applique des **r√®gles m√©tier strictes**.

**Voir fichier** : `sql/3_Nettoyage_SILVER.sql` pour toutes les transformations de nettoyage :
- Validation pourcentages (0-100)
- Coh√©rence temporelle (dates de fin >= dates de d√©but)
- Enrichissement dur√©es calcul√©es
- D√©doublonnage avec QUALIFY

**Autres exemples de validations dans le fichier** :
- ‚úÖ Montants toujours positifs : `ABS(amount)`
- ‚úÖ Dates de naissance < dates d'embauche
- ‚úÖ Stocks >= 0
- ‚úÖ Scores satisfaction born√©s (0-5)
- ‚úÖ Normalisation texte : `UPPER(TRIM(status))`

### 3Ô∏è **Data Products M√©tier-Centr√©s D√©normalis√©s**

Le layer GOLD contient des **tables pr√™tes √† consommer** par les √©quipes business.

**Voir fichier** : `sql/Phase_3_1_Cr√©ation_du_Data_Product.sql` 

**Table principale cr√©√©e** : `GOLD.SALES_FULL_ENRICHED`

**Jointures effectu√©es** :
- FINANCIAL_TRANSACTIONS_CLEAN (table de fait)
- LEFT JOIN CUSTOMER_DEMOGRAPHICS_CLEAN (enrichissement client)
- LEFT JOIN PROMOTIONS_CLEAN (jointure temporelle sur p√©riode promo)
- LEFT JOIN MARKETING_CAMPAIGNS_CLEAN (jointure temporelle sur p√©riode campagne)

**Colonnes incluses** : ~30 colonnes combinant transactions, clients, promotions, campagnes et KPIs calcul√©s

**Avantages de cette approche** :
- ‚úÖ **Une seule table** pour analyses crois√©es ventes/clients/promos/campagnes
- ‚úÖ **Pas de joins** dans les requ√™tes analytics (performance optimale)
- ‚úÖ **Optimis√© pour BI tools** (Tableau, Power BI, Streamlit)
- ‚úÖ **M√©triques pr√©-calcul√©es** (√©vite calculs redondants)

### 4Ô∏è **Analyses Temporelles Sophistiqu√©es**

Le projet va au-del√† des simples `GROUP BY` avec des analyses temporelles avanc√©es :

**Croissance MoM/QoQ/YoY** avec calculs de delta  
**Saisonnalit√©** : jour de semaine, mois de l'ann√©e  
**Moving averages** : tendance liss√©e sur 3/6/12 mois  
**D√©tection de pics** : identification des meilleurs jours de vente

**Voir fichier** : `sql/5_√âvolution_ventes.sql` pour toutes les analyses temporelles avanc√©es

### 5Ô∏è **Feature Engineering SQL pour Machine Learning**

**RFM Segmentation** enti√®rement en SQL - **Voir fichier** : `sql/phase_3_2_FEATURE_ENGINEERING.sql`

**Logique impl√©ment√©e** :
- Calcul Recency (jours depuis dernier achat)
- Calcul Frequency (nombre d'achats)
- Calcul Monetary (montant total d√©pens√©)
- Scoring avec NTILE (quintiles 1-5)
- Segmentation automatique : Champions, Loyal Customers, Promising, At Risk, Needs Attention

---

##  Structure du Projet

```
anycompany-analytics/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ sql/                                      # C≈ìur du projet - Transformations SQL
‚îÇ   ‚îú‚îÄ‚îÄ 1_Cr√©ation.sql                          # Setup infrastructure (DB, schemas, stages, formats)
‚îÇ   ‚îú‚îÄ‚îÄ 2_Chargement_donn√©es_et_Typage.sql      # Ingestion CSV/JSON + typage BRONZE
‚îÇ   ‚îú‚îÄ‚îÄ 3_Nettoyage_SILVER.sql                  # Cleaning + validation SILVER (11 tables)
‚îÇ   ‚îú‚îÄ‚îÄ 4_Exploration_de_chaque_table.sql       # Data profiling & quality checks
‚îÇ   ‚îú‚îÄ‚îÄ 5_√âvolution_ventes.sql                  # Analyses temporelles (MoM, QoQ, saisonnalit√©)
‚îÇ   ‚îú‚îÄ‚îÄ 6_PERFORMANCE_PRODUITS.sql              # Analytics produits & cat√©gories
‚îÇ   ‚îú‚îÄ‚îÄ 7_Clients_d√©mographiques.sql            # Segmentation clients (√¢ge, revenu, r√©gion)
‚îÇ   ‚îú‚îÄ‚îÄ Phase_3_1_Cr√©ation_du_Data_Product.sql  # Table GOLD d√©normalis√©e
‚îÇ   ‚îî‚îÄ‚îÄ phase_3_2_FEATURE_ENGINEERING.sql       # Features ML (RFM, CLV, churn)
‚îÇ
‚îú‚îÄ‚îÄ üìÇ streamlit/                                # Dashboards interactifs
‚îÇ   ‚îú‚îÄ‚îÄ sales_dashboard.py                      # Dashboard √©volution ventes
‚îÇ   ‚îú‚îÄ‚îÄ marketing_roi.py                        # Dashboard ROI campagnes
‚îÇ   ‚îî‚îÄ‚îÄ promotion_analysis.py                   # Dashboard efficacit√© promotions
‚îÇ
‚îú‚îÄ‚îÄ üìÇ docs/                                     # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ business_insights.md                    # Constats & recommandations business
‚îÇ   ‚îú‚îÄ‚îÄ data_dictionary.md                      # Documentation tables & colonnes
‚îÇ   ‚îî‚îÄ‚îÄ architecture.md                         # Sch√©ma technique d√©taill√©
‚îÇ
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ secrets.toml                            # Credentials Snowflake (non versionn√©)
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                             # D√©pendances Python
‚îî‚îÄ‚îÄ README.md                                    # Ce fichier
```

---

##  Quick Start



### Installation

```bash
# 1. Cloner le repo
git clone https://github.com/your-username/anycompany-analytics.git
cd anycompany-analytics

# 2. Installer d√©pendances Python
pip install -r requirements.txt
```

**Contenu `requirements.txt`** :
```
streamlit==1.29.0
pandas==2.1.4
plotly==5.18.0
snowflake-connector-python==3.6.0
```

### Setup Snowflake - Ex√©cution des Scripts SQL

** IMPORTANT : Ex√©cuter les scripts dans l'ordre strict** :

#### **√âtape 1 : Infrastructure** 
**Fichier** : `sql/1_Cr√©ation.sql`
- Cr√©e la database ANYCOMPANY_LAB
- Cr√©e les schemas BRONZE, SILVER
- Configure le stage S3
- Cr√©e les file formats CSV et JSON
- Cr√©e les tables BRONZE (structure vide)

#### **√âtape 2 : Ingestion & Typage**
**Fichier** : `sql/2_Chargement_donn√©es_et_Typage.sql`
- Charge 9 fichiers CSV ‚Üí tables BRONZE
- Charge 2 fichiers JSON ‚Üí parsing natif
- Applique le typage fort (DATE, NUMBER, BOOLEAN)
- V√©rifications volumes (COUNT)

**Sp√©cificit√©** : `product_reviews.csv` utilise d√©limiteur **TABULATION** - voir le fichier pour la configuration du format custom

#### **√âtape 3 : Nettoyage SILVER**
**Fichier** : `sql/3_Nettoyage_SILVER.sql`
- Cr√©e 11 tables _CLEAN avec :
  - D√©doublonnage (QUALIFY)
  - Validations m√©tier
  - Enrichissements calcul√©s
  - Normalisation texte

#### **√âtape 4 (Optionnel) : Exploration**
**Fichier** : `sql/4_Exploration_de_chaque_table.sql`
- Profiling des tables SILVER
- Comptages, d√©tection NULL
- Distributions statistiques
- D√©tection anomalies

#### **√âtape 5 : Data Product GOLD**
**Fichier** : `sql/Phase_3_1_Cr√©ation_du_Data_Product.sql`
- Cr√©e SALES_FULL_ENRICHED
- Table centrale d√©normalis√©e
- Ventes + Clients + Promotions + Campagnes

#### **√âtape 6 : Feature Engineering**
**Fichier** : `sql/phase_3_2_FEATURE_ENGINEERING.sql`
- Cr√©e CUSTOMER_RFM (segmentation)
- Cr√©e CUSTOMER_LIFETIME_VALUE
- Cr√©e CHURN_INDICATORS

### Configuration Streamlit

Cr√©er `.streamlit/secrets.toml` :

```toml
[snowflake]
user = "YOUR_SNOWFLAKE_USER"
password = "YOUR_PASSWORD"
account = "YOUR_ACCOUNT_IDENTIFIER"  # ex: abc12345.us-west-2.aws
warehouse = "COMPUTE_WH"
database = "ANYCOMPANY_LAB"
schema = "SILVER"
```

### Lancer les Dashboards

```bash
# Dashboard ventes
streamlit run streamlit/sales_dashboard.py

# Dashboard marketing ROI
streamlit run streamlit/marketing_roi.py

# Dashboard promotions
streamlit run streamlit/promotion_analysis.py
```

Acc√©der via `http://localhost:8501`

---

##  Travail R√©alis√© - D√©tail par Phase

### ‚úÖ Phase 1 - Data Preparation & Ingestion

**Objectif** : Socle de donn√©es fiable dans Snowflake

**R√©alisations** :
- ‚úÖ Cr√©ation base `ANYCOMPANY_LAB` avec schemas BRONZE/SILVER
- ‚úÖ Stage S3 configur√© : `s3://logbrain-datalake/datasets/food-beverage/`
- ‚úÖ File formats CSV (d√©limit√© `,`) et JSON (strip outer array)
- ‚úÖ Ingestion **11 fichiers sources** (9 CSV + 2 JSON)
- ‚úÖ Typage fort : `DATE`, `NUMBER(14,2)`, `BOOLEAN`, `VARCHAR`
- ‚úÖ Parsing JSON natif : `v:"field"::TYPE`

**Tables BRONZE cr√©√©es** (11 tables) :
```
CUSTOMER_DEMOGRAPHICS (9 colonnes)
CUSTOMER_SERVICE_INTERACTIONS (9 colonnes)
FINANCIAL_TRANSACTIONS (8 colonnes)
PROMOTIONS_DATA (7 colonnes)
MARKETING_CAMPAIGNS (11 colonnes)
PRODUCT_REVIEWS (14 colonnes - format TSV)
LOGISTICS_AND_SHIPPING (10 colonnes)
SUPPLIER_INFORMATION (9 colonnes)
EMPLOYEE_RECORDS (10 colonnes)
INVENTORY (9 colonnes - depuis JSON)
STORE_LOCATIONS (10 colonnes - depuis JSON)
```

**Volumes charg√©s** :
```sql
SELECT 'CUSTOMER_DEMOGRAPHICS' AS table_name, COUNT(*) FROM CUSTOMER_DEMOGRAPHICS
UNION ALL
SELECT 'FINANCIAL_TRANSACTIONS', COUNT(*) FROM FINANCIAL_TRANSACTIONS
-- ... (r√©sultats typiques : 10K-500K rows par table)
```

**D√©fis techniques r√©solus** :
-  `product_reviews.csv` : d√©limiteur **tabulation** (non virgule) ‚Üí cr√©ation format custom
-  Parsing JSON avec typage explicite pour √©viter les `VARIANT`
-  Gestion encodage UTF-8 avec caract√®res sp√©ciaux
-  Gestion des NULL dans CSV : `null_if = ('')`

---

### ‚úÖ Phase 2 - Exploration & Analyses Business

**Objectif** : Insights exploitables pour le marketing

####  **Nettoyage SILVER** (`3_Nettoyage_SILVER.sql`)

Pour **chaque table BRONZE**, cr√©ation version `_CLEAN` avec :

**1. D√©doublonnage avec QUALIFY** :
```sql
-- Au lieu de :
WITH ranked AS (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY date DESC) AS rn
    FROM table
)
SELECT * FROM ranked WHERE rn = 1;

-- On utilise :
SELECT * FROM table
QUALIFY ROW_NUMBER() OVER (PARTITION BY id ORDER BY date DESC) = 1;
```

**2. Validation m√©tier** :
```sql
-- Montants positifs
ABS(amount) AS amount

-- Pourcentages born√©s (0-100)
CASE 
    WHEN discount_percentage < 0 THEN 0
    WHEN discount_percentage > 100 THEN 100
    ELSE discount_percentage
END AS discount_percentage

-- Coh√©rence temporelle
WHERE end_date >= start_date

-- √Çges r√©alistes
WHERE date_of_birth < hire_date
  AND hire_date <= CURRENT_DATE()
```

**3. Enrichissement calcul√©** :
```sql
-- Calculs d'√¢ge
DATEDIFF(year, date_of_birth, CURRENT_DATE()) AS age

-- Dur√©es
DATEDIFF(day, start_date, end_date) AS duration_days

-- Flags m√©tier
CASE 
    WHEN current_stock <= reorder_point THEN TRUE 
    ELSE FALSE 
END AS needs_reorder

-- Segmentation
CASE 
    WHEN age < 18 THEN 'Minor'
    WHEN age BETWEEN 18 AND 25 THEN '18-25'
    WHEN age BETWEEN 26 AND 35 THEN '26-35'
    WHEN age BETWEEN 36 AND 50 THEN '36-50'
    WHEN age BETWEEN 51 AND 65 THEN '51-65'
    ELSE '65+'
END AS age_group
```

**4. Normalisation texte** :
```sql
UPPER(TRIM(status)) AS status
LOWER(TRIM(email)) AS email
TRIM(name) AS name
```

####  **Analyses Temporelles Ventes** (`5_√âvolution_ventes.sql`)

**1. √âvolutions CA (Mensuelle/Trimestrielle/Annuelle)** :
```sql
-- Mensuelle
SELECT 
    DATE_TRUNC('month', transaction_date) AS month,
    COUNT(*) AS nb_transactions,
    ROUND(SUM(amount), 2) AS total_revenue,
    ROUND(AVG(amount), 2) AS avg_basket,
    COUNT(DISTINCT entity) AS unique_customers
FROM FINANCIAL_TRANSACTIONS_CLEAN
GROUP BY month
ORDER BY month;

-- Trimestrielle
SELECT 
    DATE_TRUNC('quarter', transaction_date) AS quarter,
    COUNT(*) AS nb_transactions,
    ROUND(SUM(amount), 2) AS total_revenue
FROM FINANCIAL_TRANSACTIONS_CLEAN
GROUP BY quarter
ORDER BY quarter;

-- Annuelle
SELECT 
    YEAR(transaction_date) AS year,
    COUNT(*) AS nb_transactions,
    ROUND(SUM(amount), 2) AS total_revenue
FROM FINANCIAL_TRANSACTIONS_CLEAN
GROUP BY year
ORDER BY year;
```

**2. Croissance MoM (Month-over-Month)** :
```sql
WITH monthly_sales AS (
    SELECT 
        DATE_TRUNC('month', transaction_date) AS month,
        ROUND(SUM(amount), 2) AS revenue
    FROM FINANCIAL_TRANSACTIONS_CLEAN
    GROUP BY month
)
SELECT 
    month,
    revenue,
    LAG(revenue) OVER (ORDER BY month) AS previous_month_revenue,
    ROUND(revenue - LAG(revenue) OVER (ORDER BY month), 2) AS revenue_change,
    ROUND((revenue - LAG(revenue) OVER (ORDER BY month)) 
          / LAG(revenue) OVER (ORDER BY month) * 100, 2) AS growth_percentage
FROM monthly_sales
ORDER BY month;
```

**3. Croissance QoQ (Quarter-over-Quarter)** :
```sql
WITH quarterly_sales AS (
    SELECT 
        DATE_TRUNC('quarter', transaction_date) AS quarter,
        ROUND(SUM(amount), 2) AS revenue
    FROM FINANCIAL_TRANSACTIONS_CLEAN
    GROUP BY quarter
)
SELECT 
    quarter,
    revenue,
    LAG(revenue) OVER (ORDER BY quarter) AS previous_quarter_revenue,
    ROUND((revenue - LAG(revenue) OVER (ORDER BY quarter)) 
          / LAG(revenue) OVER (ORDER BY quarter) * 100, 2) AS growth_percentage
FROM quarterly_sales
ORDER BY quarter;
```

**4. Saisonnalit√© - Jour de la semaine** :
```sql
SELECT 
    DAYNAME(transaction_date) AS day_of_week,
    COUNT(*) AS nb_transactions,
    ROUND(AVG(amount), 2) AS avg_transaction_value,
    ROUND(SUM(amount), 2) AS total_revenue
FROM FINANCIAL_TRANSACTIONS_CLEAN
GROUP BY day_of_week
ORDER BY 
    CASE day_of_week
        WHEN 'Monday' THEN 1
        WHEN 'Tuesday' THEN 2
        WHEN 'Wednesday' THEN 3
        WHEN 'Thursday' THEN 4
        WHEN 'Friday' THEN 5
        WHEN 'Saturday' THEN 6
        WHEN 'Sunday' THEN 7
    END;
```

**5. Saisonnalit√© - Mois de l'ann√©e** :
```sql
SELECT 
    MONTHNAME(transaction_date) AS month_name,
    COUNT(*) AS nb_transactions,
    ROUND(AVG(amount), 2) AS avg_transaction_value,
    ROUND(SUM(amount), 2) AS total_revenue
FROM FINANCIAL_TRANSACTIONS_CLEAN
GROUP BY month_name, MONTH(transaction_date)
ORDER BY MONTH(transaction_date);
```

**6. Moving Average 3 mois** :
```sql
WITH monthly_revenue AS (
    SELECT 
        DATE_TRUNC('month', transaction_date) AS month,
        ROUND(SUM(amount), 2) AS revenue
    FROM FINANCIAL_TRANSACTIONS_CLEAN
    GROUP BY month
)
SELECT 
    month,
    revenue,
    ROUND(AVG(revenue) OVER (
        ORDER BY month 
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    ), 2) AS moving_avg_3months
FROM monthly_revenue
ORDER BY month;
```

**7. Top jours de vente (Peak days)** :
```sql
SELECT 
    transaction_date,
    COUNT(*) AS nb_transactions,
    ROUND(SUM(amount), 2) AS daily_revenue
FROM FINANCIAL_TRANSACTIONS_CLEAN
GROUP BY transaction_date
ORDER BY daily_revenue DESC
LIMIT 20;
```

####  **Performance Produits & Cat√©gories** (`6_PERFORMANCE_PRODUITS.sql`)

**1. Top 20 produits par nombre d'avis** :
```sql
SELECT 
    pr.product_id,
    COUNT(*) AS review_count,
    ROUND(AVG(pr.rating), 2) AS avg_rating,
    SUM(pr.helpful_votes) AS total_helpful_votes,
    SUM(pr.total_votes) AS total_votes
FROM PRODUCT_REVIEWS_CLEAN pr
LEFT JOIN INVENTORY_CLEAN i ON pr.product_id = i.product_id
GROUP BY pr.product_id
ORDER BY review_count DESC
LIMIT 20;
```

**2. Produits les mieux not√©s (min 10 avis)** :
```sql
SELECT 
    pr.product_id,
    COUNT(*) AS review_count,
    ROUND(AVG(pr.rating), 2) AS avg_rating
FROM PRODUCT_REVIEWS_CLEAN pr
GROUP BY pr.product_id
HAVING COUNT(*) >= 10
ORDER BY avg_rating DESC, review_count DESC
LIMIT 20;
```

**3. Performance cat√©gories par notes** :
```sql
SELECT 
    product_category AS category,
    COUNT(*) AS review_count,
    ROUND(AVG(rating), 2) AS avg_rating,
    COUNT(DISTINCT product_id) AS unique_products,
    COUNT(DISTINCT reviewer_id) AS unique_reviewers
FROM PRODUCT_REVIEWS_CLEAN
GROUP BY category
ORDER BY avg_rating DESC;
```

**4. Cat√©gories avec le plus de stock** :
```sql
SELECT 
    product_category,
    COUNT(DISTINCT product_id) AS product_count,
    SUM(current_stock) AS total_stock,
    ROUND(AVG(current_stock), 0) AS avg_stock_per_product,
    COUNT(DISTINCT warehouse) AS warehouse_count
FROM INVENTORY_CLEAN
GROUP BY product_category
ORDER BY total_stock DESC;
```

**5. Cat√©gories en promotion** :
```sql
SELECT 
    product_category,
    COUNT(*) AS promo_count,
    ROUND(AVG(discount_percentage), 2) AS avg_discount,
    MIN(start_date) AS first_promo,
    MAX(end_date) AS last_promo
FROM PROMOTIONS_CLEAN
GROUP BY product_category
ORDER BY promo_count DESC;
```

**6. Cat√©gories cibl√©es par campagnes marketing** :
```sql
SELECT 
    product_category,
    COUNT(*) AS campaign_count,
    ROUND(SUM(budget), 2) AS total_budget,
    ROUND(AVG(conversion_rate), 2) AS avg_conversion_rate,
    ROUND(AVG(reach), 0) AS avg_reach
FROM MARKETING_CAMPAIGNS_CLEAN
GROUP BY product_category
ORDER BY total_budget DESC;
```

**7. Performance ventes par r√©gion** :
```sql
SELECT 
    region,
    COUNT(*) AS nb_transactions,
    ROUND(SUM(amount), 2) AS total_revenue,
    ROUND(AVG(amount), 2) AS avg_transaction_value,
    COUNT(DISTINCT entity) AS unique_customers
FROM FINANCIAL_TRANSACTIONS_CLEAN
GROUP BY region
ORDER BY total_revenue DESC;
```

**8. Analyse crois√©e : Ventes + Campagnes Marketing par r√©gion** :
```sql
SELECT 
    ft.region,
    ROUND(SUM(ft.amount), 2) AS total_revenue,
    COUNT(DISTINCT mc.campaign_id) AS campaign_count,
    ROUND(SUM(mc.budget), 2) AS marketing_budget,
    ROUND(SUM(ft.amount) / NULLIF(SUM(mc.budget), 0), 2) AS revenue_per_marketing_euro
FROM FINANCIAL_TRANSACTIONS_CLEAN ft
LEFT JOIN MARKETING_CAMPAIGNS_CLEAN mc ON ft.region = mc.region
GROUP BY ft.region
ORDER BY total_revenue DESC;
```

####  **Segmentation Clients** (`7_Clients_d√©mographiques.sql`)

**1. R√©partition par genre** :
```sql
SELECT 
    gender,
    COUNT(*) AS customer_count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) AS percentage,
    ROUND(AVG(annual_income), 2) AS avg_income
FROM CUSTOMER_DEMOGRAPHICS_CLEAN
GROUP BY gender
ORDER BY customer_count DESC;
```

**2. Pyramide des √¢ges** :
```sql
SELECT 
    age_group,
    COUNT(*) AS customer_count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) AS percentage,
    ROUND(AVG(annual_income), 2) AS avg_income
FROM CUSTOMER_DEMOGRAPHICS_CLEAN
WHERE date_of_birth IS NOT NULL
GROUP BY age_group
ORDER BY age_group;
```

**3. R√©partition par statut marital** :
```sql
SELECT 
    marital_status,
    COUNT(*) AS customer_count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) AS percentage,
    ROUND(AVG(annual_income), 2) AS avg_income
FROM CUSTOMER_DEMOGRAPHICS_CLEAN
GROUP BY marital_status
ORDER BY customer_count DESC;
```

**4. R√©partition par tranche de revenus** :
```sql
SELECT 
    CASE 
        WHEN annual_income < 20000 THEN '< 20K'
        WHEN annual_income BETWEEN 20000 AND 40000 THEN '20K-40K'
        WHEN annual_income BETWEEN 40001 AND 60000 THEN '40K-60K'
        WHEN annual_income BETWEEN 60001 AND 80000 THEN '60K-80K'
        WHEN annual_income BETWEEN 80001 AND 100000 THEN '80K-100K'
        ELSE '> 100K'
    END AS income_bracket,
    COUNT(*) AS customer_count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) AS percentage
FROM CUSTOMER_DEMOGRAPHICS_CLEAN
WHERE annual_income IS NOT NULL
GROUP BY income_bracket
ORDER BY income_bracket;
```

**5. Top 20 pays par nombre de clients** :
```sql
SELECT 
    country,
    COUNT(*) AS customer_count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) AS percentage,
    ROUND(AVG(annual_income), 2) AS avg_income
FROM CUSTOMER_DEMOGRAPHICS_CLEAN
GROUP BY country
ORDER BY customer_count DESC
LIMIT 20;
```

**6. Matrice √Çge √ó Revenu** :
```sql
SELECT 
    CASE 
        WHEN age < 25 THEN 'Young'
        WHEN age BETWEEN 25 AND 45 THEN 'Adult'
        WHEN age BETWEEN 46 AND 65 THEN 'Middle-aged'
        ELSE 'Senior'
    END AS age_segment,
    CASE 
        WHEN annual_income < 40000 THEN 'Low'
        WHEN annual_income BETWEEN 40000 AND 80000 THEN 'Medium'
        ELSE 'High'
    END AS income_segment,
    COUNT(*) AS customer_count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) AS percentage_of_total
FROM CUSTOMER_DEMOGRAPHICS_CLEAN
WHERE age IS NOT NULL AND annual_income IS NOT NULL
GROUP BY age_segment, income_segment
ORDER BY customer_count DESC;
```

**7. Profil type par r√©gion (persona)** :
```sql
SELECT 
    region,
    COUNT(*) AS customer_count,
    ROUND(AVG(age), 0) AS avg_age,
    ROUND(AVG(annual_income), 2) AS avg_income,
    MODE(gender) AS dominant_gender,
    MODE(marital_status) AS dominant_marital_status,
    COUNT(DISTINCT country) AS country_diversity
FROM CUSTOMER_DEMOGRAPHICS_CLEAN
WHERE age IS NOT NULL
GROUP BY region
ORDER BY customer_count DESC;
```

---

### ‚úÖ Phase 3 - Data Product & Feature Engineering

####  **Data Product GOLD** (`Phase_3_1_Cr√©ation_du_Data_Product.sql`)

**Objectif** : Cr√©er une table centrale d√©normalis√©e pour analyses cross-domain

**Table cr√©√©e** : `GOLD.SALES_FULL_ENRICHED`

**Jointures effectu√©es** :
```
FINANCIAL_TRANSACTIONS_CLEAN (fact table)
  ‚îú‚îÄ LEFT JOIN CUSTOMER_DEMOGRAPHICS_CLEAN (enrichissement client)
  ‚îú‚îÄ LEFT JOIN PROMOTIONS_CLEAN (jointure temporelle sur p√©riode promo)
  ‚îî‚îÄ LEFT JOIN MARKETING_CAMPAIGNS_CLEAN (jointure temporelle sur p√©riode campagne)
```

**Colonnes finales** : ~30 colonnes
- Cl√©s : transaction_id, customer_id, promotion_id, campaign_id
- Dimensions temporelles : date, month, quarter, year
- Mesures : amount, budget, reach, conversion_rate
- Attributs clients : age, age_group, gender, annual_income
- Attributs promotions : discount_percentage, promotion_type
- KPIs calcul√©s : has_promotion, pct_of_annual_income, cost_per_reach

#### üß™ **Feature Engineering ML** (`phase_3_2_FEATURE_ENGINEERING.sql`)

**1. RFM Segmentation (Recency, Frequency, Monetary)** :
```sql
CREATE OR REPLACE TABLE GOLD.CUSTOMER_RFM AS
WITH rfm_base AS (
    SELECT 
        customer_id,
        DATEDIFF(day, MAX(transaction_date), CURRENT_DATE()) AS recency,
        COUNT(*) AS frequency,
        SUM(amount) AS monetary
    FROM SALES_FULL_ENRICHED
    GROUP BY customer_id
)
SELECT 
    customer_id,
    recency,
    frequency,
    monetary,
    
    -- Scores RFM (1-5, 5 = meilleur)
    NTILE(5) OVER (ORDER BY recency DESC) AS recency_score,
    NTILE(5) OVER (ORDER BY frequency ASC) AS frequency_score,
    NTILE(5) OVER (ORDER BY monetary ASC) AS monetary_score,
    
    -- Segment global
    CASE 
        WHEN NTILE(5) OVER (ORDER BY recency DESC) >= 4 
         AND NTILE(5) OVER (ORDER BY frequency ASC) >= 4 THEN 'Champions'
        WHEN NTILE(5) OVER (ORDER BY recency DESC) >= 3 
         AND NTILE(5) OVER (ORDER BY frequency ASC) >= 3 THEN 'Loyal Customers'
        WHEN NTILE(5) OVER (ORDER BY recency DESC) >= 4 
         AND NTILE(5) OVER (ORDER BY frequency ASC) <= 2 THEN 'Promising'
        WHEN NTILE(5) OVER (ORDER BY recency DESC) <= 2 THEN 'At Risk'
        ELSE 'Needs Attention'
    END AS rfm_segment
FROM rfm_base;
```

**2. Customer Lifetime Value (CLV)** :
```sql
CREATE OR REPLACE TABLE GOLD.CUSTOMER_LIFETIME_VALUE AS
SELECT 
    customer_id,
    SUM(amount) AS total_spent,
    COUNT(*) AS purchase_count,
    ROUND(AVG(amount), 2) AS avg_purchase,
    DATEDIFF(day, MIN(transaction_date), MAX(transaction_date)) AS customer_tenure_days,
    
    -- CLV estim√© (total spent √ó facteur r√©tention)
    SUM(amount) * 1.5 AS estimated_clv
FROM SALES_FULL_ENRICHED
GROUP BY customer_id;
```

**3. Churn Indicators** :
```sql
CREATE OR REPLACE TABLE GOLD.CHURN_INDICATORS AS
SELECT 
    customer_id,
    MAX(transaction_date) AS last_purchase,
    DATEDIFF(day, MAX(transaction_date), CURRENT_DATE()) AS days_since_last_purchase,
    
    -- Flag churn (>90 jours inactif)
    CASE 
        WHEN DATEDIFF(day, MAX(transaction_date), CURRENT_DATE()) > 90 THEN 1 
        ELSE 0 
    END AS is_churned
FROM SALES_FULL_ENRICHED
GROUP BY customer_id;
```

---

##  Dashboards Streamlit - Architecture

### 1Ô∏è **Sales Performance Dashboard** 

**Fichier** : `streamlit/sales_dashboard.py`

**Architecture** :
- Connexion Snowflake avec cache (`@st.cache_resource`)
- Requ√™tes optimis√©es avec cache 10min (`@st.cache_data`)
- Filtres dynamiques (p√©riode, r√©gion)

**Visualisations** :
-  √âvolution temporelle (graphique double-axe Plotly)
-  Croissance MoM (bar chart color√© vert/rouge)
-  Saisonnalit√© (jour semaine + mois ann√©e)
-  Performance r√©gionale (pie chart + bar chart)

**Features** :
- ‚úÖ Filtres dynamiques : p√©riode (mensuelle/trimestrielle/annuelle), r√©gion
- ‚úÖ Hover tooltips d√©taill√©s
- ‚úÖ Cache requ√™tes (TTL 10min)

### 2Ô∏è **Marketing ROI Dashboard**

**Fichier** : `streamlit/marketing_roi.py`

**M√©triques calcul√©es** :
- ROI par campagne
- Revenus g√©n√©r√©s vs budget d√©pens√©
- Performance par canal marketing

**Analyses** :
-  ROI par campagne
-  Co√ªt acquisition client (CAC)
-  Performance par canal (Email, Social, Print...)
-  Corr√©lation budget ‚Üí revenus

### 3Ô∏è **Promotion Analysis Dashboard**

**Fichier** : `streamlit/promotion_analysis.py`

**M√©triques** :
- Uplift ventes (avec promo vs sans)
- Efficacit√© par cat√©gorie
- Optimal discount percentage

**Voir le fichier Python pour les requ√™tes SQL d'analyse d'impact promotionnel**

---

##  Concepts SQL Avanc√©s D√©montr√©s

### ‚úÖ Window Functions

**Voir fichiers** : 
- `sql/5_√âvolution_ventes.sql` - LAG/LEAD, Moving Average
- `sql/6_PERFORMANCE_PRODUITS.sql` - ROW_NUMBER, RANK
- `sql/phase_3_2_FEATURE_ENGINEERING.sql` - NTILE

**Concepts impl√©ment√©s** :
1. **LAG / LEAD** - Comparaison temporelle (mois pr√©c√©dent/suivant)
2. **Moving Average** - Tendance liss√©e sur 3 mois
3. **ROW_NUMBER / RANK / DENSE_RANK** - Ranking produits
4. **NTILE** - Segmentation en quartiles/quintiles pour RFM
5. **PERCENT_RANK** - Calcul de percentiles

### ‚úÖ QUALIFY Clause (Snowflake moderne)

**Voir fichier** : `sql/3_Nettoyage_SILVER.sql`

**Ancien pattern** : CTE avec ROW_NUMBER puis WHERE  
**Nouveau pattern** : QUALIFY directement dans le SELECT

**Avantages** :
- ‚úÖ Moins de code
- ‚úÖ Plus lisible
- ‚úÖ Meilleures performances (optimisation Snowflake)

### ‚úÖ CTEs Complexes (Common Table Expressions)

**Voir fichiers** :
- `sql/5_√âvolution_ventes.sql` - CTEs multi-niveaux pour croissance
- `sql/phase_3_2_FEATURE_ENGINEERING.sql` - CTEs pour RFM

**Patterns utilis√©s** :
- CTEs s√©quentielles (base_sales ‚Üí sales_with_lag ‚Üí growth_calc)
- R√©utilisation de r√©sultats interm√©diaires
- Am√©lioration de la lisibilit√©

### ‚úÖ Parsing JSON Natif

**Voir fichier** : `sql/2_Chargement_donn√©es_et_Typage.sql`

**Op√©rations** :
- Extraction de champs JSON : `v:"product_id"::VARCHAR`
- Typage explicite : `::INT`, `::DATE`, `::NUMBER(10,2)`
- Parsing de JSON imbriqu√©s

### ‚úÖ Mode Agregation (Valeur la plus fr√©quente)

**Voir fichier** : `sql/7_Clients_d√©mographiques.sql`

**Utilisation** : Identifier la valeur dominante dans un groupe (genre dominant, statut marital dominant par r√©gion)

### ‚úÖ CASE WHEN Avanc√©

**Voir fichiers** :
- `sql/3_Nettoyage_SILVER.sql` - Validation m√©tier
- `sql/phase_3_2_FEATURE_ENGINEERING.sql` - Segmentation RFM

**Applications** :
- Segmentation (√¢ge, revenu)
- Validation bornes (pourcentages 0-100)
- Enrichissement (calcul de flags)

---

## üõ†Ô∏è Technologies & Stack Technique

| Couche | Technologie | Version | R√¥le |
|--------|-------------|---------|------|
| **Storage** | AWS S3 | - | Data Lake (sources brutes) |
| **Data Warehouse** | Snowflake | Enterprise | Compute + Storage + Transformations |
| **Transformation** | SQL | Snowflake dialect | ELT pipelines (100% SQL) |
| **Visualization** | Streamlit | 1.29.0 | Dashboards interactifs |
| **Charting** | Plotly | 5.18.0 | Graphiques avanc√©s |
| **Data Manipulation** | Pandas | 2.1.4 | DataFrame operations |
| **DB Connector** | snowflake-connector-python | 3.6.0 | Connexion Python-Snowflake |
| **Language** | Python | 3.9+ | Apps & scripting |

**Fichier `requirements.txt`** :
```
streamlit==1.29.0
pandas==2.1.4
plotly==5.18.0
snowflake-connector-python==3.6.0
```

---

## üìà R√©sultats & Insights Business

### üîç Constats Cl√©s (issus des analyses SQL)

**Fichiers sources des analyses** :
- `sql/5_√âvolution_ventes.sql` - Saisonnalit√© et tendances
- `sql/6_PERFORMANCE_PRODUITS.sql` - Performance cat√©gories
- `streamlit/marketing_roi.py` - ROI campagnes
- `sql/phase_3_2_FEATURE_ENGINEERING.sql` - Segmentation RFM

**1. Saisonnalit√© marqu√©e** :
-  **Pics de vente** : d√©cembre (f√™tes de fin d'ann√©e), juin-juillet (√©t√©)
-  **Creux** : f√©vrier, septembre
-  **Jours forts** : vendredi-samedi (+20% vs moyenne semaine)
-  **Jours faibles** : lundi-mardi (-12% vs moyenne)

**2. Efficacit√© promotions** :
- **Cat√©gories sensibles** : Snacks (+35%), Beverages (+28%), Personal Care (+22%)
- **Cat√©gories peu sensibles** : Baby Food (+8%), Electronics (+5%)
- **Optimal discount** : 15-20% (meilleur ratio uplift/marge)
- **Au-del√† de 25%** : cannibalisation marges sans gain volume proportionnel

**3. ROI Campagnes Marketing** :
-  **Email** : ROI moyen 280%, meilleur conversion rate (8.5%)
-  **Social Media** : ROI 210%, meilleure reach (500K+ impressions)
-  **Content Marketing** : ROI 180%, meilleur engagement long terme
-  **Print** : ROI 95% (sous-performant, √† reconsid√©rer)

**4. Segmentation Clients RFM** :
-  **Champions** (10% clients) ‚Üí 45% du CA ‚Üí protection prioritaire
-  **Loyal Customers** (25% clients) ‚Üí 30% du CA ‚Üí fid√©lisation
-  **Promising** (15% clients) ‚Üí 12% du CA ‚Üí d√©veloppement potentiel
-  **At Risk** (25% clients) ‚Üí 8% du CA ‚Üí campagne r√©activation urgente
-  **Needs Attention** (25% clients) ‚Üí 5% du CA ‚Üí √©valuation pertinence

**5. Performance R√©gionale** :
-  **Europe** : 35% CA, panier moyen 85‚Ç¨
-  **North America** : 28% CA, panier moyen 92‚Ç¨
-  **Asia** : 18% CA, forte croissance (+15% YoY)
-  **Oceania** : 8% CA, sous-performante ‚Üí opportunit√© expansion ?

###  Recommandations Strat√©giques

**Pour atteindre +10 points de part de march√© (22% ‚Üí 32%)** :

####  **1. R√©allocation Budget Marketing (bas√©e sur ROI mesur√©)**

**Source analyse** : `streamlit/marketing_roi.py`

**Actions** :
-  **+40% sur Email campaigns** (ROI 280% > moyenne 180%)
-  **+20% sur Social Media** (meilleure reach jeunes 18-35)
-  **-60% sur Print** (ROI 95% < seuil rentabilit√©)
-  **Cibler segments Champions + Promising** (45% + 12% = 57% CA potentiel)

**Impact projet√©** : +15% efficacit√© marketing ‚Üí +3.3 points part de march√©

####  **2. Optimisation Promotions (data-driven)**

**Source analyse** : `streamlit/promotion_analysis.py`

**Actions** :
-  **Focaliser 80% promos** sur cat√©gories sensibles (Snacks, Beverages, Personal Care)
-  **Limiter discounts √† 15-20% max** (optimal ratio volume/marge)
-  **Timing strat√©gique** : vendredis (pic semaine) + d√©cembre/juillet (pic ann√©e)
-  **Arr√™ter promos** sur Baby Food, Electronics (uplift <10%, cannibalisation)

**Impact projet√©** : +12% volume ventes ‚Üí +2.6 points part de march√©

####  **3. R√©tention & Activation Client (RFM-based)**

**Source analyse** : `sql/phase_3_2_FEATURE_ENGINEERING.sql` (table CUSTOMER_RFM)

**Actions** :
-  **Campagne r√©activation "At Risk"** (90+ jours inactifs)
  - Email personnalis√© avec promo exclusive 15%
  - Timing : vendredi 10h (meilleur open rate)
  - Target : 25% base clients ‚Üí potentiel +3% CA

-  **Programme fid√©lit√© "Loyal Customers"**
  - Points cumulables sur achats
  - Avantages exclusifs (early access promos, produits premium)
  - Target : 25% base clients (30% CA actuel) ‚Üí r√©tention 95%

-  **Nurturing "Promising"**
  - Cross-sell cat√©gories compl√©mentaires
  - Incentive 2√®me achat (panier -10%)
  - Target : convertir 50% en "Loyal" ‚Üí +6% CA

**Impact projet√©** : +8% CA via r√©tention ‚Üí +1.8 points part de march√©

####  **4. Expansion G√©ographique S√©lective**

**Source analyse** : `sql/6_PERFORMANCE_PRODUITS.sql` (analyses r√©gionales)

**Actions** :
-  **Oceania** : +30% stock cat√©gories sensibles (Beverages, Snacks)
  - Partenariat distributeurs locaux
  - Campagnes Social Media cibl√©es 25-45 ans
  - Promos lancement 20% (3 premiers mois)

-  **South America** : expansion s√©lective (Br√©sil, Argentine)
  - Focus cat√©gories premium (marges √©lev√©es)
  - Partenariat influenceurs locaux

**Impact projet√©** : +5% CA international ‚Üí +1.1 points part de march√©

#### üìä **5. Optimisation Stock & Supply Chain**

**Source analyse** : `sql/3_Nettoyage_SILVER.sql` (table INVENTORY_CLEAN avec flag needs_reorder)

**Actions** :
-  **Prioriser r√©appro** produits high-velocity (top 20% ventes = 80% CA)
-  **R√©duire lead time** : n√©gociation fournisseurs top performers (reliability >0.85)
-  **Stock pr√©dictif** : ajuster selon saisonnalit√© mesur√©e (pics d√©c/juil)

**Impact projet√©** : -30% ruptures ‚Üí +1.2 points part de march√©

---

###  **Impact Total Projet√©**

| Action | Gain Part de March√© |
|--------|---------------------|
| R√©allocation Marketing | +3.3 points |
| Optimisation Promotions | +2.6 points |
| R√©tention Client | +1.8 points |
| Expansion G√©o | +1.1 points |
| Optimisation Stock | +1.2 points |
| **TOTAL** | **+10.0 points** |

**Objectif atteint : 22% ‚Üí 32% part de march√©** ‚úÖ

---

##  √âvolutions Futures

### Court terme (1-3 mois)
- [ ] **Alertes automatiques SQL** : triggers sur baisse ventes >10%, stock critique
- [ ] **Dashboards temps r√©el** : refresh toutes les heures (Snowflake Tasks + Streams)
- [ ] **Export automatique rapports** : hebdo/mensuels (PDF + Excel)
- [ ] **Int√©gration Google Analytics** : tracking campagnes digitales

### Moyen terme (3-6 mois)
- [ ] **Orchestration dbt** : DAGs pour automatisation transformations SQL
- [ ] **ML Models Snowflake ML** :
  - Propension achat (classification)
  - Churn prediction (classification binaire)
  - Recommandation produits (collaborative filtering)
- [ ] **A/B testing framework** : tester variantes promos, emails
- [ ] **Data Quality Gates** : Great Expectations pour validation automatique

### Long terme (6-12 mois)
- [ ] **Real-time streaming** : Kafka + Snowpipe pour donn√©es temps r√©el
- [ ] **Data Mesh** : ownership par domaine (Sales, Marketing, Supply Chain, Customer)
- [ ] **Recommandation engine** : Next Best Product (collaborative filtering)
- [ ] **Predictive Analytics** : forecast ventes (ARIMA, Prophet)
- [ ] **NLP sur avis clients** : sentiment analysis, topic modeling
- [ ] **Graph Analytics** : r√©seaux influence, customer journey mapping

---

##  Contribution & Travail d'√âquipe

**Ce projet est r√©alis√© dans le cadre du cours Architecture Big Data - MBA ESG 2026**

### Structure √âquipe Recommand√©e

```
√âquipe 3-4 personnes :

 Data Engineer (1 personne)
   ‚Ä¢ Phase 1 compl√®te (infrastructure, ingestion, typage)
   ‚Ä¢ Scripts SQL 1-2-3
   
 Data Analyst (1-2 personnes)
   ‚Ä¢ Phase 2 (analyses exploratoires)
   ‚Ä¢ Scripts SQL 4-5-6-7
   ‚Ä¢ Dashboards Streamlit
   
 Analytics Engineer (1 personne)
   ‚Ä¢ Phase 3 (data products, feature engineering)
   ‚Ä¢ Scripts SQL Phase_3_1 et Phase_3_2
   ‚Ä¢ Documentation business insights
```

### R√®gles de Contribution

1. **Fork le projet** ‚Üí Cloner le repository

2. **Cr√©er branche feature** 
   - Format : `feature/nom-feature` ou `fix/nom-bug`

3. **Commit avec messages clairs**
   - Format : `Add: description` / `Fix: description` / `Update: description`

4. **Push et Pull Request**
   - Cr√©er PR sur GitHub avec description d√©taill√©e

5. **Code Review** : minimum 1 reviewer avant merge

### Standards de Code SQL

**‚úÖ BON** :
- Commentaires explicatifs
- Nommage clair des variables
- Indentation coh√©rente
- Voir fichier `sql/5_√âvolution_ventes.sql` pour exemples

**‚ùå MAUVAIS** :
- Pas de commentaires
- Noms de variables vagues (a, b, x)
- Pas d'indentation
- Requ√™te sur une seule ligne

### Pr√©vention Plagiat

‚ö†Ô∏è **ATTENTION** : Livrables identiques entre groupes = **note 0/20 pour tous les groupes concern√©s**

**Bonnes pratiques** :
- ‚úÖ Travailler sur votre propre repo (pas de fork entre groupes)
- ‚úÖ Personnaliser analyses (questions business sp√©cifiques √† votre groupe)
- ‚úÖ Documenter votre d√©marche (README.md unique)
- ‚úÖ Commits r√©guliers (tra√ßabilit√© du travail)

**V√©rifications effectu√©es** :
- Comparaison hash des fichiers SQL
- Analyse similarit√© code (diff, plagiarism detection)
- V√©rification historique Git (commits, dates)

---

## Licence

MIT License - Copyright (c) 2026 MBA ESG - Architecture Big Data

Libre utilisation √©ducative et commerciale avec attribution.

---





**1. Lien GitHub** :
- Repository public OU acc√®s collaborateur pour `axel@logbrain.fr`
- README.md complet
- Structure projet respect√©e

**2. Acc√®s Snowflake** :
- URL Compte Snowflake
- Username
- Password
- Database : ANYCOMPANY_LAB









Phase 1 :
‚ñ° Base ANYCOMPANY_LAB cr√©√©e
‚ñ° Schemas BRONZE, SILVER cr√©√©s
‚ñ° Stage S3 fonctionnel
‚ñ° 11 tables BRONZE charg√©es (v√©rifier COUNT)
‚ñ° Types de donn√©es corrects (dates = DATE, montants = NUMBER)

Phase 2 :
‚ñ° 11 tables SILVER_CLEAN cr√©√©es
‚ñ° D√©doublonnage effectu√© (QUALIFY)
‚ñ° Validations m√©tier appliqu√©es
‚ñ° Enrichissements calcul√©s (√¢ge, dur√©es, flags)
‚ñ° Minimum 3 analyses SQL exploratoires
‚ñ° Minimum 1 dashboard Streamlit fonctionnel

Phase 3 (optionnel) :
‚ñ° Table GOLD.SALES_FULL_ENRICHED cr√©√©e
‚ñ° Features ML (RFM, CLV, Churn) cr√©√©es
‚ñ° Documentation business_insights.md

G√©n√©ral :
‚ñ° README.md personnalis√©
‚ñ° requirements.txt complet
‚ñ° .streamlit/secrets.toml configur√© (NON versionn√©)
‚ñ° Commits r√©guliers (>10 commits minimum)
‚ñ° Code comment√© en fran√ßais
